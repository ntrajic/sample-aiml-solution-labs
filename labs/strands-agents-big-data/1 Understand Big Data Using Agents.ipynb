{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### AI Agents and Big Data Analytics - Better Together\n",
        "Up to 80% of the data that enterprises generate is unstructured and semi-structured data. Your AI Agents will provide richer insights by querying TBs of data stored in S3 data lake.<br><br>\n",
        "Do AI developers need to learn data engineering skills such as distributed spark processing, metadata catalogs, complex distributed SQL queries, NoSQL queries etc to query TBs and PBs of data in data lakes?<br><br>\n",
        "Fortunately, the answer is no. You just need to know the basics. You can use Strands Agents, built-in strands tools such as use_aws, and MCP server such as AWS Data Processing MCP Server to accomplish what most data engineers can, while writing few lines of code.<br><br>\n",
        "This notebook will help you bridge the gap between a data engineer who knows how to use AWS Glue, Amazon Athena with an AI engineer who knows how to use Strands SDK, Amazon Bedrock. You will be able to do sophisticated analytics with natural language questions.<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### GOAL - Perform big data analytics using AI Agents without writing big data analytics code. <br>\n",
        "The AI Agent will use built-in strands tools and MCP server to get the job done. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### In this Notebook\n",
        "You will learn how to use AI Agents to discover metadata of big-data stored in parquet files using Agent tools and MCP server that in turn uses AWS Glue and Amazon Athena.<br><br>\n",
        "We will use NYC Taxi and Limousine Commission (TLC) data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Environment Setup\n",
        "Install required dependencies for the notebook including Strands SDK, AWS SDK, and MCP client libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-16T01:21:29.858994Z",
          "iopub.status.busy": "2025-10-16T01:21:29.858158Z",
          "iopub.status.idle": "2025-10-16T01:21:50.992576Z",
          "shell.execute_reply": "2025-10-16T01:21:50.991600Z",
          "shell.execute_reply.started": "2025-10-16T01:21:29.858962Z"
        }
      },
      "outputs": [],
      "source": [
        "# Install all required packages from requirements.txt\n",
        "!pip install -r requirements.txt --quiet --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up a session\n",
        "import boto3\n",
        "\n",
        "##### CHOOSE THE MOST FIT AWS CREDENTIAL FOR YOUR AWS ENVIRONMENT \n",
        "\n",
        "# Option1: Default session\n",
        "session = boto3.session.Session()\n",
        "\n",
        "# Option2: Using AWS profile defined in ~/.aws/config\n",
        "#session = boto3.session.Session(profile_name=\"<YOUR-AWS-PROFILE-NAME>\")\n",
        "\n",
        "# Option3: Explicit credentials\n",
        "#session = boto3.session.Session(\n",
        "#    aws_access_key_id='YOUR_ACCESS_KEY_ID',\n",
        "#    aws_secret_access_key='YOUR_SECRET_ACCESS_KEY',\n",
        "#    aws_session_token='YOUR_SESSION_TOKEN' \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-16T01:22:20.694540Z",
          "iopub.status.busy": "2025-10-16T01:22:20.694265Z",
          "iopub.status.idle": "2025-10-16T01:22:20.909348Z",
          "shell.execute_reply": "2025-10-16T01:22:20.908508Z",
          "shell.execute_reply.started": "2025-10-16T01:22:20.694505Z"
        }
      },
      "outputs": [],
      "source": [
        "# Lab configurations\n",
        "import os\n",
        "\n",
        "# CloudFormation Stack Name - change it as needed\n",
        "STACK_NAME = \"AwsLabBigDataAgentStack\"\n",
        "\n",
        "# Extract session data\n",
        "import boto3\n",
        "\n",
        "region = session.region_name\n",
        "sts_client = session.client(\"sts\")\n",
        "response = sts_client.get_caller_identity()\n",
        "account_id = response.get(\"Account\")\n",
        "print(\"Sesseion = \", region, account_id)\n",
        "\n",
        "credentials = session.get_credentials()\n",
        "os.environ[\"AWS_ACCESS_KEY_ID\"] = credentials.access_key\n",
        "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = credentials.secret_key\n",
        "os.environ[\"AWS_SESSION_TOKEN\"] = credentials.token\n",
        "os.environ[\"AWS_REGION\"] = region\n",
        "\n",
        "\n",
        "# Setup the logging\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(\n",
        "    level=logging.DEBUG,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    datefmt='%m/%d %H:%M:%S',\n",
        "    filename='strands_debug.log'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Pipeline using Strands Agent\n",
        "### Overview\n",
        "\n",
        "This agent downloads data from a URL and uploads it to an S3 bucket using dynamic partitioning.\n",
        "### Capabilities\n",
        "\n",
        "    Downloads data from specified URLs\n",
        "    Validates downloaded content\n",
        "    Uploads data to S3 with dynamic partitioning\n",
        "    Handles error scenarios gracefully\n",
        "    Provides logging and status updates\n",
        "\n",
        "### Prerequisites\n",
        "\n",
        "    AWS credentials configured\n",
        "    Required Python packages:\n",
        "        strands\n",
        "        boto3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Use Cloudformation template to create S3 bucket"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-16T01:54:55.899610Z",
          "iopub.status.busy": "2025-10-16T01:54:55.899046Z",
          "iopub.status.idle": "2025-10-16T01:57:56.767774Z",
          "shell.execute_reply": "2025-10-16T01:57:56.767143Z",
          "shell.execute_reply.started": "2025-10-16T01:54:55.899580Z"
        }
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# This cell will deploy AWS resources including S3 bucket and IAM roles. It requires about 3 mins to complete.\n",
        "from utils.deploy_cfn import deploy_infrastructure\n",
        "deploy_infrastructure(STACK_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import Dependencies and AWS Configuration\n",
        "Import required libraries and configure AWS settings for the data processing workflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-16T01:59:45.100340Z",
          "iopub.status.busy": "2025-10-16T01:59:45.099525Z",
          "iopub.status.idle": "2025-10-16T01:59:45.535040Z",
          "shell.execute_reply": "2025-10-16T01:59:45.534338Z",
          "shell.execute_reply.started": "2025-10-16T01:59:45.100304Z"
        }
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import os, time, boto3, json\n",
        "from strands import Agent, tool\n",
        "from strands.models import BedrockModel\n",
        "from strands_tools import use_aws, file_write, file_read, file_write, sleep, python_repl\n",
        "from datetime import datetime\n",
        "from pprint import pprint\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import Optional, List\n",
        "\n",
        "# Bypass tool consent for automated execution\n",
        "os.environ[\"BYPASS_TOOL_CONSENT\"] = \"true\"\n",
        "# Specify that if python_repl tool is used, it shouldnt wait for user interaction\n",
        "os.environ[\"PYTHON_REPL_INTERACTIVE\"] = \"false\"\n",
        "\n",
        "model_list = ['deepseek.v3-v1:0', \n",
        "            'qwen.qwen3-coder-30b-a3b-v1:0',\n",
        "            'us.anthropic.claude-3-7-sonnet-20250219-v1:0',\n",
        "            'us.anthropic.claude-sonnet-4-20250514-v1:0',\n",
        "            'openai.gpt-oss-20b-1:0',\n",
        "            'openai.gpt-oss-120b-1:0',\n",
        "            'us.anthropic.claude-haiku-4-5-20251001-v1:0']\n",
        "\n",
        "# We will use the following model in Strands Agent\n",
        "model_id = \"us.anthropic.claude-haiku-4-5-20251001-v1:0\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data ingestion using Strands Agents\n",
        "\n",
        "Build a strands agents to download data from a URL and then upload that data into S3 bucket with data partitioning enabled. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-16T02:18:34.465592Z",
          "iopub.status.busy": "2025-10-16T02:18:34.464884Z",
          "iopub.status.idle": "2025-10-16T02:19:50.310165Z",
          "shell.execute_reply": "2025-10-16T02:19:50.308935Z",
          "shell.execute_reply.started": "2025-10-16T02:18:34.465563Z"
        },
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Create a data ingestion agent.\n",
        "\n",
        "data_upload_agent = Agent(\n",
        "    model=BedrockModel(model_id=model_id, temperature=0.3, boto_session=session),\n",
        "    system_prompt=f\"\"\"You are an expert data engineer specializing in AWS data pipeline operations.\n",
        "\n",
        "TASK OVERVIEW:\n",
        "Download files from provided URLs and upload them to an S3 bucket with proper partitioning.\n",
        "\n",
        "SPECIFIC REQUIREMENTS:\n",
        "1. Download files from the given URLs to local storage in download folder\n",
        "2. Extract S3 bucket name and other resources from stack outputs (created by CloudFormation stack: {STACK_NAME}) in {region} region\n",
        "3. Verify the S3 bucket exists \n",
        "3. Upload files to the S3 bucket with appropriate partitioning structure. \n",
        "4. Use table name: data\n",
        "\n",
        "EXECUTION GUIDELINES:\n",
        "- Check S3 bucket availability before attempting uploads\n",
        "- Implement proper error handling for downloads and uploads\n",
        "- Use efficient partitioning strategy (e.g., by year/month/day if date fields exist)\n",
        "- Verify file integrity after download\n",
        "- Provide clear status updates on progress\n",
        "- Handle large files appropriately to avoid memory issues\n",
        "- Execute tasks sequentially without user intervention\n",
        "\n",
        "TOOLS AVAILABLE:\n",
        "- use_aws: For AWS operations\n",
        "- python_repl: For data processing and file operations (ALWAYS use interactive=False)\n",
        "\n",
        "Complete the task systematically in non-interactive mode and report final results.\"\"\",\n",
        "    tools=[use_aws, python_repl]\n",
        ")\n",
        "\n",
        "\n",
        "# File URLs\n",
        "file_urls = [\n",
        "    \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2025-05.parquet\",\n",
        "    \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2025-06.parquet\", \n",
        "    \"https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2025-05.parquet\",\n",
        "    \"https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2025-06.parquet\"\n",
        "]\n",
        "\n",
        "# Direct, conversational request\n",
        "data_upload_request = f\"\"\"\n",
        "Hi! I need you to help me with a data upload task.\n",
        "\n",
        "Here's what I need:\n",
        "1. Get the S3 bucket name from CloudFormation stack \"{STACK_NAME}\"\n",
        "2. Download these 4 files (only if they don't already exist locally)\n",
        "\n",
        "{file_urls}\n",
        "\n",
        "3. Upload them to S3 with this partition structure:\n",
        "   - Extract taxi type, year, month from filename\n",
        "   - Use S3 key: data/taxi_class=[type]/year=[year]/month=[month]/[filename]\n",
        "\"\"\"\n",
        "\n",
        "print(f'Starting data download')\n",
        "\n",
        "response = data_upload_agent(data_upload_request)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The first step is to discover the metadata and catalog it. <br><br>\n",
        "AWS Glue can help crawl the data, extract the metadata, and create a catalog using AWS Glue Catalog.<br><br>\n",
        "Let's NOT write code to create AWS Glue jobs. Instead, let's write a SYSTEM PROMPT that will instruct an AI Agent to do this job for us.<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Catalog Data in S3 as Glue Catalog Database and Tables\n",
        "Create a Strands Agent that uses use_aws strands tool to create AWS Glue crawlers and catalog the S3 data automatically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-16T02:22:56.552857Z",
          "iopub.status.busy": "2025-10-16T02:22:56.551576Z",
          "iopub.status.idle": "2025-10-16T02:22:56.558425Z",
          "shell.execute_reply": "2025-10-16T02:22:56.557728Z",
          "shell.execute_reply.started": "2025-10-16T02:22:56.552822Z"
        }
      },
      "outputs": [],
      "source": [
        "from utils_big_data import load_system_prompt_from_file\n",
        "# Let's load the system prompt from file\n",
        "crawl_task = load_system_prompt_from_file(\"crawl_task_prompt.txt\", stack_name = STACK_NAME)\n",
        "crawl_task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-16T02:22:58.546676Z",
          "iopub.status.busy": "2025-10-16T02:22:58.546010Z",
          "iopub.status.idle": "2025-10-16T02:24:31.171666Z",
          "shell.execute_reply": "2025-10-16T02:24:31.170985Z",
          "shell.execute_reply.started": "2025-10-16T02:22:58.546648Z"
        }
      },
      "outputs": [],
      "source": [
        "# Initialize an LLM model via Bedrock\n",
        "model = BedrockModel(model_id=model_id, temperature=0.1, boto_session=session)  # Low temperature for consistent structured output\n",
        "\n",
        "# Create the Strands Agent. \n",
        "# use_aws tool will create a glue crawler and create glue catalog db and table schema\n",
        "# sleep tool will help the agent wait until the glue crwler job is finished.\n",
        "# This task might take a few minutes. You can go to AWS Console and see the Glue Crawler jobs that are being created.\n",
        "crawl_agent = Agent(model=model, tools=[use_aws, sleep])\n",
        "\n",
        "crawl_response = crawl_agent(crawl_task)\n",
        "print(crawl_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-16T02:26:36.977704Z",
          "iopub.status.busy": "2025-10-16T02:26:36.977423Z",
          "iopub.status.idle": "2025-10-16T02:26:36.981631Z",
          "shell.execute_reply": "2025-10-16T02:26:36.980897Z",
          "shell.execute_reply.started": "2025-10-16T02:26:36.977684Z"
        }
      },
      "outputs": [],
      "source": [
        "from utils_big_data import print_tokens_costs\n",
        "\n",
        "# Let's print the token costs\n",
        "print_tokens_costs(crawl_response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extract Glue Database and Tables Names\n",
        "Parse the agent response to extract the Glue database name and Table names for use in subsequent queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-16T02:26:38.859032Z",
          "iopub.status.busy": "2025-10-16T02:26:38.858235Z",
          "iopub.status.idle": "2025-10-16T02:26:41.392546Z",
          "shell.execute_reply": "2025-10-16T02:26:41.391867Z",
          "shell.execute_reply.started": "2025-10-16T02:26:38.859002Z"
        }
      },
      "outputs": [],
      "source": [
        "# Strands Agenst allows you to extract information in a structured dictionary output.\n",
        "\n",
        "# Define the structured template\n",
        "class GlueDbTableInfo(BaseModel):\n",
        "    catalog_db_name: str = Field(description=\"Name of the Glue Catalog Database\")\n",
        "    catalog_table_names: List[str] = Field(description=\"List of glue table names\")\n",
        "\n",
        "# Use the structured_output method\n",
        "glue_db_table_info = crawl_agent.structured_output(GlueDbTableInfo, f\"Extract Glue Catalog Database name and the list of glue table names\")\n",
        "print(glue_db_table_info)\n",
        "catalog_db_name = glue_db_table_info.catalog_db_name\n",
        "catalog_table_names = glue_db_table_info.catalog_table_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "AWS Labs has created many MCP servers for AI Agent developers to consume. One of them is AWS data processing MCP server: https://awslabs.github.io/mcp/servers/aws-dataprocessing-mcp-server <br><br>\n",
        "This MCP server exposes AI Agent tools to do operations on big data using AWS Glue, Amazon EMR, and Amazon Athena.<br><br>\n",
        "The best part is that we don't have to know the intricacies of these services other than high level basics.<br><br>\n",
        "Let's create an MCP client for this MCP server."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MCP Client Setup\n",
        "We wll get tools exposed by an MCP server to discover partition columns and keys from the data in S3.<br><br>\n",
        "Initialize the AWS Data Processing MCP server client to provide AI agents with AWS Glue, EMR, and Athena capabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-16T02:26:44.835570Z",
          "iopub.status.busy": "2025-10-16T02:26:44.834908Z",
          "iopub.status.idle": "2025-10-16T02:26:45.772604Z",
          "shell.execute_reply": "2025-10-16T02:26:45.771909Z",
          "shell.execute_reply.started": "2025-10-16T02:26:44.835541Z"
        }
      },
      "outputs": [],
      "source": [
        "# Import MCP client libraries\n",
        "from mcp import stdio_client, StdioServerParameters\n",
        "from strands.tools.mcp import MCPClient\n",
        "import boto3\n",
        "\n",
        "# Create MCP client for AWS data processing server\n",
        "# This provides tools for Glue, EMR, and Athena operations\n",
        "data_mcp_client = MCPClient(lambda: stdio_client(\n",
        "    StdioServerParameters(\n",
        "        command=\"uvx\",  # Use uvx to run the MCP server\n",
        "        args= [\n",
        "            \"awslabs.aws-dataprocessing-mcp-server@latest\",\n",
        "            \"--allow-write\",  # Enable write operations\n",
        "        ],\n",
        "        env= {\n",
        "            \"AWS_ACCESS_KEY_ID\": os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
        "            \"AWS_SECRET_ACCESS_KEY\": os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
        "            \"AWS_SESSION_TOKEN\": os.environ[\"AWS_SESSION_TOKEN\"],\n",
        "            \"FASTMCP_LOG_LEVEL\": \"ERROR\",  # Minimize logging noise\n",
        "            \"AWS_REGION\": os.environ[\"AWS_REGION\"]      # Set AWS region\n",
        "      }\n",
        "    )\n",
        "))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-16T02:26:50.780691Z",
          "iopub.status.busy": "2025-10-16T02:26:50.780104Z",
          "iopub.status.idle": "2025-10-16T02:26:50.826968Z",
          "shell.execute_reply": "2025-10-16T02:26:50.826230Z",
          "shell.execute_reply.started": "2025-10-16T02:26:50.780661Z"
        }
      },
      "outputs": [],
      "source": [
        "import boto3\n",
        "\n",
        "sts_client = boto3.client('sts')\n",
        "response = sts_client.get_caller_identity()\n",
        "\n",
        "arn = response['Arn']\n",
        "# Example: arn:aws:sts::123456789012:assumed-role/MyRole/MySession\n",
        "if \"assumed-role\" in arn:\n",
        "    role_name = arn.split('/')[-2]\n",
        "    print(f\"IAM Role Name: {role_name}\")\n",
        "else:\n",
        "    print(\"Not an assumed role session.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Print the tools offered by the MCP Server\n",
        "This will help you understand what the tools do. This is extracted by looking at doc strings or tool spec of the tools.<br><br>\n",
        "Agents send the doc string or the tool spec of the tools to the LLM along with a task.<br>\n",
        "This helps LLM to reason and decide which tool to use for which task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-16T02:27:14.034275Z",
          "iopub.status.busy": "2025-10-16T02:27:14.033628Z",
          "iopub.status.idle": "2025-10-16T02:27:19.346846Z",
          "shell.execute_reply": "2025-10-16T02:27:19.346164Z",
          "shell.execute_reply.started": "2025-10-16T02:27:14.034246Z"
        },
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Connect to MCP client and execute task\n",
        "import json\n",
        "\n",
        "tools_char_count = 0\n",
        "tool_count = 0\n",
        "with data_mcp_client:\n",
        "    # Get available tools from the MCP server\n",
        "    data_tools = data_mcp_client.list_tools_sync()\n",
        "    # first_tool = data_tools[0]\n",
        "    # print(dir(first_tool))    \n",
        "    # Iterate through each tool\n",
        "    for tool in data_tools:\n",
        "        print(f\"Tool: {tool.tool_name}\")\n",
        "        if hasattr(tool, 'tool_spec'):            \n",
        "            print(f\"Tool: {tool.tool_spec['description']}\")\n",
        "            # print(json.dumps(tool.tool_spec, indent=2)) # uncomment this to see function parameters and what they mean   \n",
        "            tools_char_count += len(json.dumps(tool.tool_spec))\n",
        "            tool_count += 1\n",
        "        print(\"-\" * 50)\n",
        "print(f\"The number of characters in the spec of all the {tool_count} tools = {tools_char_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Understand the content that the tables have and what the columns mean. \n",
        "Store this in a file and pass it to the agent. This will help agent construct SQL queries properly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-16T02:27:27.083736Z",
          "iopub.status.busy": "2025-10-16T02:27:27.083431Z",
          "iopub.status.idle": "2025-10-16T02:32:05.372513Z",
          "shell.execute_reply": "2025-10-16T02:32:05.371841Z",
          "shell.execute_reply.started": "2025-10-16T02:27:27.083714Z"
        }
      },
      "outputs": [],
      "source": [
        "# Get the definition of the table and columns and store it in a file.\n",
        "table_def_system_prompt = f\"\"\"\n",
        "You are an expert AWS data analyst assistant specializing in querying data stored in S3 data lakes using AWS Glue and Amazon Athena.\n",
        "\n",
        "## IMPORTANT: \n",
        "- For any long running jobs, sleep for 20 seconds and check status until the job is finished. Do not give up until the job actually finishes.\n",
        "\"\"\"\n",
        "query_in = f\"\"\"Query the database {catalog_db_name}, identify the tables in it, identify the columns and their types. Gather more business context of what the database, tables, and the columns are storing by sampling a few rows of data. Then store the database name, table names, their purpose; and column name and its type and purpose in a json file named metadata.json.\"\"\"\n",
        "\n",
        "column_def_response = \"\"\n",
        "# Get the data processing tools from MCP server\n",
        "with data_mcp_client:\n",
        "    data_tools = data_mcp_client.list_tools_sync()\n",
        "    curated_data_tools = ['manage_aws_athena_query_executions', 'manage_aws_glue_tables']\n",
        "\n",
        "    # Extract just the tools that we need.\n",
        "    filtered_tools = [tool for tool in data_tools if tool.tool_name in curated_data_tools]\n",
        "    filtered_tools += [file_write, file_read, sleep]\n",
        "\n",
        "    # Pass the system prompt, the LLM we use with bedrock, and all the tools to the agent\n",
        "    column_def = Agent(system_prompt = table_def_system_prompt, model=model, tools=filtered_tools)\n",
        "    # Invoke the agent with each of the query\n",
        "    column_def_response = column_def(query_in)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "thalaiva",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}