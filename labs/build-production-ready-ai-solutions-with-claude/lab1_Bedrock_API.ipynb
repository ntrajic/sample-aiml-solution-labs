{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Amazon Bedrock Workshop: Getting Started with Amazon Bedrock APIs using Claude Sonnet 3.7\n",
        "\n",
        "Welcome to this hands-on workshop where you'll learn how to interact with Amazon Bedrock APIs using Anthropic's Claude Sonnet 3.7 model. \n",
        "\n",
        "## What is Amazon Bedrock?\n",
        "\n",
        "[Amazon Bedrock](https://aws.amazon.com/bedrock/) is a comprehensive, secure, and flexible service for building generative AI applications and agents. Amazon Bedrock connects you to leading foundation models (FMs), services to deploy and operate agents, and tools for fine-tuning, safeguarding, and optimizing models along with knowledge bases to connect applications to your latest data so that you have everything you need to quickly move from experimentation to real-world deployment.\n",
        "\n",
        "### Key Benefits:\n",
        "- **Easy Integration**: Single API to access multiple foundation models\n",
        "- **Serverless**: No infrastructure to manage\n",
        "- **Security**: Your data stays within your AWS account\n",
        "- **Scalability**: Automatically scales based on demand\n",
        "\n",
        "\n",
        "## Anthropic Claude\n",
        "[Claude](https://docs.claude.com/en/docs/about-claude/models/overview) is a family of state-of-the-art large language models developed by Anthropic. This guide introduces our models and compares their performance with legacy models.\n",
        "\n",
        "\n",
        "## Lab Overview\n",
        "\n",
        "In this lab, you'll learn several Amazon Bedrock APIs:\n",
        "1. Basic API calls using `invoke_model`\n",
        "2. Streaming responses with `invoke_model_with_response_stream`\n",
        "3. Conversational interactions using `converse` \n",
        "4. Streaming conversations with `converse_stream`\n",
        "\n",
        "Let's explore each one with practical examples!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Prerequisites\n",
        "\n",
        "First, let's install the required Python library. We'll use `boto3`, the AWS SDK for Python, to interact with Bedrock APIs.\n",
        "\n",
        "**Note**: Make sure you have enabled Claude Sonnet model access in your AWS Bedrock console."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-03T20:50:17.671363Z",
          "iopub.status.busy": "2025-10-03T20:50:17.671086Z",
          "iopub.status.idle": "2025-10-03T20:50:19.213784Z",
          "shell.execute_reply": "2025-10-03T20:50:19.213114Z",
          "shell.execute_reply.started": "2025-10-03T20:50:17.671343Z"
        }
      },
      "outputs": [],
      "source": [
        "%pip install boto3 -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Required Libraries\n",
        "\n",
        "Now let's import the libraries we'll need throughout this workshop.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-03T20:50:23.922195Z",
          "iopub.status.busy": "2025-10-03T20:50:23.921925Z",
          "iopub.status.idle": "2025-10-03T20:50:24.123608Z",
          "shell.execute_reply": "2025-10-03T20:50:24.123080Z",
          "shell.execute_reply.started": "2025-10-03T20:50:23.922171Z"
        }
      },
      "outputs": [],
      "source": [
        "import boto3\n",
        "import json\n",
        "\n",
        "# Create a Bedrock client (replace region with where you enabled Bedrock)\n",
        "region_name = boto3.Session().region_name\n",
        "bedrock = boto3.client(\"bedrock-runtime\", region_name=region_name)\n",
        "\n",
        "print(\"Bedrock client created successfully!\")\n",
        "print(f\"Region: {bedrock.meta.region_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Our Context Data\n",
        "\n",
        "Throughout this lab, we'll use Amazon's Returns & Refunds policy as our context. This provides a practical, real-world scenario for our API examples.\n",
        "\n",
        "**What this cell does:**\n",
        "- Defines a context string containing Amazon's return policy information\n",
        "- This context will be used in our prompts to give the AI model relevant information\n",
        "- Using consistent context helps demonstrate how the same information can be used across different API methods\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-03T20:50:28.942997Z",
          "iopub.status.busy": "2025-10-03T20:50:28.942666Z",
          "iopub.status.idle": "2025-10-03T20:50:28.946495Z",
          "shell.execute_reply": "2025-10-03T20:50:28.945910Z",
          "shell.execute_reply.started": "2025-10-03T20:50:28.942976Z"
        }
      },
      "outputs": [],
      "source": [
        "# Context from Amazon Returns & Refunds FAQ\n",
        "returns_context = \"\"\"\n",
        "Amazon Returns & Refunds Policy:\n",
        "- Most items can be returned within 30 days of receipt of delivery.\n",
        "- You can initiate a return from 'Your Orders'.\n",
        "- Refunds are issued to the original payment method once the item is received.\n",
        "- Some items may not be returnable (e.g., perishable goods, digital products).\n",
        "- A-to-Z Guarantee covers purchases from third-party sellers if the item doesn't arrive or isn't as described.\n",
        "\"\"\"\n",
        "\n",
        "print(\"Context data defined:\")\n",
        "print(returns_context)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Basic API Invocation with `invoke_model`\n",
        "\n",
        "The `invoke_model` API is the most basic way to interact with Bedrock models. It sends a request and waits for the complete response.\n",
        "\n",
        "### Understanding the Parameters\n",
        "\n",
        "- **`modelId`**: Specifies which model to use (Claude Sonnet 3.7 in our case)\n",
        "- **`body`**: Contains the request payload as JSON, including:\n",
        "  - `anthropic_version`: API version for Anthropic models\n",
        "  - `max_tokens`: Maximum number of tokens in the response\n",
        "  - `messages`: Array of conversation messages with roles and content\n",
        "\n",
        "### Expected Output\n",
        "You'll receive a JSON response containing the model's answer about returning items on Amazon, along with metadata like token usage and stop reason.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Understanding Inference Configuration Parameters\n",
        "\n",
        "Let's explore what each parameter does:\n",
        "\n",
        "#### `maxTokens`\n",
        "- Controls the maximum length of the response\n",
        "- Higher values allow longer responses\n",
        "- Consider your use case and cost implications\n",
        "\n",
        "#### `temperature` (0.0 - 1.0)\n",
        "- **0.0**: Deterministic, always picks the most likely next word\n",
        "- **0.3-0.5**: Good balance for most applications\n",
        "- **0.7**: More creative and varied responses (what we used above)\n",
        "- **1.0**: Very creative, potentially less coherent\n",
        "\n",
        "#### `topP` (not used above, but available)\n",
        "- Controls the diversity of word selection\n",
        "- Lower values = more focused responses\n",
        "- Higher values = more diverse responses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-03T20:50:33.794624Z",
          "iopub.status.busy": "2025-10-03T20:50:33.794345Z",
          "iopub.status.idle": "2025-10-03T20:50:38.395412Z",
          "shell.execute_reply": "2025-10-03T20:50:38.394896Z",
          "shell.execute_reply.started": "2025-10-03T20:50:33.794604Z"
        }
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"Question: How can I return an item on Amazon?\"\"\"\n",
        "\n",
        "response = bedrock.invoke_model(\n",
        "    modelId=\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
        "    body=json.dumps({\n",
        "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
        "        \"max_tokens\": 1000,\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"user\", \n",
        "                \"content\": prompt\n",
        "            }\n",
        "        ]\n",
        "    })\n",
        ")\n",
        "\n",
        "result = json.loads(response[\"body\"].read())\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Understanding the Response\n",
        "\n",
        "The response above contains several important fields:\n",
        "\n",
        "- **`content`**: Array containing the actual response text\n",
        "- **`usage`**: Token usage information (input and output tokens)\n",
        "- **`stop_reason`**: Why the model stopped generating (usually \"end_turn\")\n",
        "- **`model`**: Confirms which model was used\n",
        "\n",
        "Let's extract just the text content for easier reading:\n",
        "\n",
        "**What this cell does:**\n",
        "- Checks if the response contains content\n",
        "- Extracts the text from the first content block\n",
        "- Displays it in a formatted way\n",
        "- Shows token usage statistics\n",
        "\n",
        "**Expected outcome:** You'll see Claude's response in a clean, readable format along with information about how many tokens were used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-03T20:51:01.055197Z",
          "iopub.status.busy": "2025-10-03T20:51:01.054927Z",
          "iopub.status.idle": "2025-10-03T20:51:01.059266Z",
          "shell.execute_reply": "2025-10-03T20:51:01.058649Z",
          "shell.execute_reply.started": "2025-10-03T20:51:01.055176Z"
        }
      },
      "outputs": [],
      "source": [
        "# Extract and display just the text content\n",
        "if result.get('content') and len(result['content']) > 0:\n",
        "    response_text = result['content'][0]['text']\n",
        "    print(\"Claude's Response:\")\n",
        "    print(\"=\" * 50)\n",
        "    print(response_text)\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Tokens used: {result['usage']['input_tokens']} input, {result['usage']['output_tokens']} output\")\n",
        "else:\n",
        "    print(\"No content found in response\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Streaming Responses with `invoke_model_with_response_stream`\n",
        "\n",
        "Sometimes you want to see the response as it's being generated, especially for longer responses. This creates a better user experience in interactive applications.\n",
        "\n",
        "### When to use:\n",
        "- Interactive chat applications\n",
        "- Long responses where users want to see progress\n",
        "- Better user experience with immediate feedback\n",
        "\n",
        "### How it works:\n",
        "- Response comes in chunks\n",
        "- Each chunk contains a piece of the text\n",
        "- You process chunks as they arrive\n",
        "\n",
        "### What this cell does:\n",
        "- Uses our returns context to provide background information\n",
        "- Asks a specific question about Amazon's refund policy\n",
        "- Uses `invoke_model_with_response_stream` instead of `invoke_model`\n",
        "- Processes streaming chunks in real-time\n",
        "- Displays text as it arrives from the model\n",
        "\n",
        "### Expected Output\n",
        "You'll see the response appear word by word, simulating a typing effect. This demonstrates how streaming works in real-time applications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-03T20:51:04.700304Z",
          "iopub.status.busy": "2025-10-03T20:51:04.700033Z",
          "iopub.status.idle": "2025-10-03T20:51:06.759020Z",
          "shell.execute_reply": "2025-10-03T20:51:06.758506Z",
          "shell.execute_reply.started": "2025-10-03T20:51:04.700284Z"
        }
      },
      "outputs": [],
      "source": [
        "prompt = f\"\"\"{returns_context}\n",
        "\n",
        "Question: What is Amazon's refund policy?\n",
        "\"\"\"\n",
        "\n",
        "response = bedrock.invoke_model_with_response_stream(\n",
        "    modelId=\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
        "    body=json.dumps({\n",
        "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
        "        \"max_tokens\": 200,\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt\n",
        "            }\n",
        "        ]\n",
        "    })\n",
        ")\n",
        "\n",
        "print(\"Streaming response:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for event in response[\"body\"]:\n",
        "    if \"chunk\" in event:\n",
        "        chunk = json.loads(event[\"chunk\"][\"bytes\"].decode(\"utf-8\"))\n",
        "        if chunk[\"type\"] == \"content_block_delta\":\n",
        "            print(chunk[\"delta\"][\"text\"], end=\"\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"Streaming complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Understanding Streaming Events\n",
        "\n",
        "When using streaming, the response comes as a series of events:\n",
        "\n",
        "- **`chunk`**: Contains the actual data\n",
        "- **`type`**: Indicates the type of chunk (we want \"content_block_delta\")\n",
        "- **`delta`**: Contains the incremental text to add\n",
        "\n",
        "### Streaming vs Non-Streaming Comparison\n",
        "\n",
        "| Aspect | `invoke_model` | `invoke_model_with_response_stream` |\n",
        "|--------|----------------|------------------------------------|\n",
        "| **Response Time** | Wait for complete response | Immediate feedback |\n",
        "| **User Experience** | All-at-once | Progressive display |\n",
        "| **Complexity** | Simple | Requires event handling |\n",
        "| **Best For** | Batch processing | Interactive apps |\n",
        "| **Token Limits** | Same limits apply | Same limits apply |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Conversational API with `converse`\n",
        "\n",
        "The `converse` method provides a more structured way to handle conversations. It's designed specifically for multi-turn conversations and offers better support for system messages and conversation context.\n",
        "\n",
        "### Advantages:\n",
        "- Simpler syntax\n",
        "- Consistent across different models\n",
        "- Better support for conversations\n",
        "- Separate system and user messages\n",
        "- Built-in inference configuration\n",
        "\n",
        "### Key Features:\n",
        "- `system`: For setting context and behavior\n",
        "- `messages`: For the actual conversation\n",
        "- `inferenceConfig`: For model parameters\n",
        "\n",
        "### What this cell does:\n",
        "- Uses the `converse` method instead of `invoke_model`\n",
        "- Passes our returns context as a system message\n",
        "- Asks a question about canceling Amazon orders\n",
        "- Demonstrates the cleaner conversation structure\n",
        "\n",
        "### Expected Output\n",
        "You'll receive a response about canceling Amazon orders, with the context provided through system messages. Notice how the API structure is cleaner and more intuitive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-03T20:51:15.333789Z",
          "iopub.status.busy": "2025-10-03T20:51:15.333530Z",
          "iopub.status.idle": "2025-10-03T20:51:18.908526Z",
          "shell.execute_reply": "2025-10-03T20:51:18.907884Z",
          "shell.execute_reply.started": "2025-10-03T20:51:15.333769Z"
        }
      },
      "outputs": [],
      "source": [
        "response = bedrock.converse(\n",
        "    modelId=\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
        "    system=[{\"text\": returns_context}],\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": [{\"text\": \"How do I cancel an Amazon order?\"}]}\n",
        "    ]\n",
        ")\n",
        "\n",
        "for msg in response[\"output\"][\"message\"][\"content\"]:\n",
        "    print(msg[\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Streaming Conversations with `converse_stream`\n",
        "\n",
        "The `converse_stream` method combines the best of both worlds: the structured conversation format of `converse` with the real-time streaming of `invoke_model_with_response_stream`.\n",
        "\n",
        "### What this cell does:\n",
        "- Uses `converse_stream` for real-time streaming with conversation structure\n",
        "- Includes system context about Amazon's return policy\n",
        "- Asks about Amazon's A-to-Z Guarantee\n",
        "- Sets inference parameters to control response generation\n",
        "- Displays the response as it streams in real-time\n",
        "\n",
        "### Expected Output\n",
        "You'll see a streaming explanation of Amazon's A-to-Z Guarantee appearing in real-time, demonstrating how to combine conversational structure with streaming responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-03T20:51:22.487823Z",
          "iopub.status.busy": "2025-10-03T20:51:22.487566Z",
          "iopub.status.idle": "2025-10-03T20:51:28.241695Z",
          "shell.execute_reply": "2025-10-03T20:51:28.241154Z",
          "shell.execute_reply.started": "2025-10-03T20:51:22.487804Z"
        }
      },
      "outputs": [],
      "source": [
        "response = bedrock.converse_stream(\n",
        "    modelId=\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
        "    system=[{\"text\": returns_context}],\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": [{\"text\": \"Explain Amazon's A-to-Z Guarantee\"}]}\n",
        "    ],\n",
        "    inferenceConfig={\n",
        "        \"maxTokens\": 500,\n",
        "        \"temperature\": 0.7\n",
        "    }\n",
        ")\n",
        "\n",
        "print(\"Streaming conversational response:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for event in response[\"stream\"]:\n",
        "    if \"contentBlockDelta\" in event:\n",
        "        print(event[\"contentBlockDelta\"][\"delta\"][\"text\"], end=\"\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"Streaming conversation complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Congratulations! You've successfully completed this lab. You now have hands-on experience with:\n",
        "\n",
        "### âœ… What You've Learned:\n",
        "\n",
        "1. **Basic API Invocation** - Using `invoke_model` for simple requests\n",
        "2. **Streaming Responses** - Real-time text generation with `invoke_model_with_response_stream`\n",
        "3. **Conversational APIs** - Structured conversations using `converse`\n",
        "4. **Streaming Conversations** - Real-time chat with `converse_stream`\n",
        "\n",
        "\n",
        "Let's continue our learning with lab 2 for prompt engineering."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}