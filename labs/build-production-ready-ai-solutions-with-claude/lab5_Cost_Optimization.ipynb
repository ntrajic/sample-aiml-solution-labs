{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents Cost Optimization\n",
    "\n",
    "In this lab you will look various techniques including to optimize running Agentic AI Applications in Production for Enterprise Applications. The key components of cost in an Agentic AI application are 1) Inference Cost largely driven by the token count (in case of on-demand models) and model size (in case custom models or fine-tuned models.) 2) Knowledge Base - cost of ingesting the data and synchronizing the changes and cost of retrieving the context from the knowledgebase 3) Tool Calls - compute cost for executing the tools and the wait times associated or cost of the API if they are external 4) Invovation Cost - the compute involved in handling the requests from the users or the batch compute or the event triggering mechanisms.\n",
    "\n",
    "Various techniques to optimize cost are  as follows and each of the following would be illustrated in the steps below:\n",
    "- Dynamic Model Selection\n",
    "- Trim prompts and outputs\n",
    "- Use caching\n",
    "- Batch events for inference provided use case allows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the strands-agents library to the runtime.\n",
    "!pip install strands-agents --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup all the imports required for the rest of the notebook\n",
    "import boto3\n",
    "import json\n",
    "from strands import Agent, tool\n",
    "from strands.models import BedrockModel\n",
    "import time\n",
    "from typing import List, Dict\n",
    "import utils.lab5_tools as lab_5_utils\n",
    "# from utils import get_param_value\n",
    "import importlib\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a model id for the US Cross Region Anthropic Claude Sonnet 3.7 Model\n",
    "US_ANTHROPIC_SONNET37_MODEL_ID = \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.Session()\n",
    "\n",
    "sts = session.client('sts')\n",
    "identity = sts.get_caller_identity()\n",
    "ACCOUNT_ID = identity['Account']\n",
    "REGION = boto3.Session().region_name or 'us-west-2'\n",
    "\n",
    "print(f\"Account ID: {ACCOUNT_ID}\")\n",
    "print(f\"Region: {REGION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Model Selection\n",
    "\n",
    "Not all uses of Large Language applications require the same model size, for example a simple FAQ-style prompt could be handled by an Anthropic Claude Haiku model could be sufficient while a highly complex open-ended question or to sight-unseen scenarios with fluency and human-like understanding most likely will need a Claude Opus Model instead. Amazon Bedrock has a feature [Prompt Routing](https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-routing.html) which allows to intelligently route requests to different models within the same model family. This feature helps to optimize for both response quality and cost.\n",
    "\n",
    "By default Amazon Bedrock provides 1) Nova Prompt Router 2) Anthropic Prompt Router 3) Meta Prompt Router. In the below cell we will explore Bedrock's Prompt Routing feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Bedrock client\n",
    "bedrock_client = boto3.client(\"bedrock\", region_name=REGION)\n",
    "\n",
    "# Get List of Default Prompt Routers\n",
    "prompt_router_response = bedrock_client.list_prompt_routers(\n",
    "    type='default'\n",
    ")\n",
    "\n",
    "# Search for the Anthropic Default Prompt Router and get its ARN\n",
    "# This default prompt router model uses Claude 3 Haiku, Claude 3.5 Sonnet and switches to Claude 3.5 Sonnet as a fallback model\n",
    "prompt_router_response_summaries = prompt_router_response[\"promptRouterSummaries\"]\n",
    "anthropic_default_router = [item for item in prompt_router_response_summaries \n",
    "                            if item[\"promptRouterName\"] == \"Anthropic Prompt Router\"]\n",
    "default_anthropic_prompt_router_arn = anthropic_default_router[0][\"promptRouterArn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run same set of queries through both the Default Anthropic Prompt Router and Anthropic Claude Sonnet 3.7 Model\n",
    "bedrock_runtime_client = boto3.client('bedrock-runtime')\n",
    "\n",
    "# Pricing per 1M tokens\n",
    "INPUT_COST = 3.00\n",
    "OUTPUT_COST = 15.00\n",
    "\n",
    "# Test with different query complexities\n",
    "test_queries = [\n",
    "    \"How do I return an item?\",\n",
    "    \"Where is my order?\",\n",
    "    \"What is Amazon Prime?\",\n",
    "    \"Explain Amazon's return policy for electronics including restocking fees, original packaging requirements, and warranty implications for opened items.\",\n",
    "    \"I need help with a complex return situation involving multiple international orders shipped to different addresses with various payment methods and gift card combinations.\"\n",
    "]\n",
    "\n",
    "print(\"üöÄ YOUR CUSTOM ROUTER COST COMPARISON\\n\")\n",
    "\n",
    "# ===== ALWAYS USE SONNET =====\n",
    "print(\"üìä ALWAYS USE CLAUDE 3.7 SONNET:\")\n",
    "total_cost_always_sonnet = 0\n",
    "\n",
    "for i, query in enumerate(test_queries):\n",
    "    response, model, tokens = lab_5_utils.call_sonnet_directly(bedrock_runtime_client, US_ANTHROPIC_SONNET37_MODEL_ID, query)\n",
    "    cost = lab_5_utils.router_calculate_cost(\"sonnet\", tokens)\n",
    "    total_cost_always_sonnet += cost\n",
    "\n",
    "    print(f\"  Query {i+1}: Sonnet ‚Üí {tokens:,} tokens ‚Üí ${cost:.6f}\")\n",
    "\n",
    "print(f\"  TOTAL: ${total_cost_always_sonnet:.6f}\\n\")\n",
    "\n",
    "# ===== USE YOUR CUSTOM ROUTER =====\n",
    "print(\"‚ö° USE DEFAULT ANTHROPIC ROUTER:\")\n",
    "total_cost_with_router = 0\n",
    "\n",
    "for i, query in enumerate(test_queries):\n",
    "    response, model, tokens = lab_5_utils.call_with_your_router(bedrock_runtime_client, default_anthropic_prompt_router_arn, query)\n",
    "    cost = lab_5_utils.router_calculate_cost(model, tokens)\n",
    "    total_cost_with_router += cost\n",
    "\n",
    "    model_display = \"üü¢ Haiku (Fast)\" if model == \"haiku\" else \"üü° Sonnet (Smart)\"\n",
    "    print(f\"  Query {i+1}: {model_display} ‚Üí {tokens:,} tokens ‚Üí ${cost:.6f}\")\n",
    "\n",
    "print(f\"  TOTAL: ${total_cost_with_router:.6f}\\n\")\n",
    "\n",
    "lab_5_utils.print_prompt_router_cost_savings(total_cost_always_sonnet, total_cost_with_router)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trimming Prompts and Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Token Count both Input and Output Tokens are a key driver for cost associated with Bedrock. There are primarily System Prompts, User Prompts and Knowledge Content that contribute to the input token count. Having optimzed prompts and knowledge is not only key to have optimal costs but also is key to maintain optimal latency for users experience. In the below example you will look how an optimal user prompt results in cost savings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "bedrock = boto3.client(\"bedrock-runtime\", region_name=REGION)\n",
    "\n",
    "importlib.reload(lab_5_utils)\n",
    "\n",
    "# Amazon Customer Service Examples\n",
    "\n",
    "print(\"üìä PROMPT OPTIMIZATION - COST COMPARISON\")\n",
    "print(\"Using Amazon customer service examples...\")\n",
    "print()\n",
    "\n",
    "# ===== TEST UNOPTIMIZED PROMPTS =====\n",
    "print(\"‚ùå UNOPTIMIZED PROMPTS (Verbose & Wasteful):\")\n",
    "unoptimized_total = 0\n",
    "\n",
    "for i, prompt in enumerate(lab_5_utils.unoptimized_prompts):\n",
    "    input_tokens, output_tokens, response = lab_5_utils.get_response(bedrock, US_ANTHROPIC_SONNET37_MODEL_ID, prompt)\n",
    "    cost = lab_5_utils.trim_prompt_calculate_cost(input_tokens, output_tokens)\n",
    "    unoptimized_total += cost\n",
    "\n",
    "    print(f\"  Query {i+1}:\")\n",
    "    print(f\"    Input: {input_tokens} tokens\")\n",
    "    print(f\"    Output: {output_tokens} tokens\") \n",
    "    print(f\"    Cost: ${cost:.6f}\")\n",
    "    print(f\"    Response: {response[:60]}...\")\n",
    "    print()\n",
    "\n",
    "print(f\"üìä TOTAL UNOPTIMIZED: ${unoptimized_total:.6f}\")\n",
    "print()\n",
    "\n",
    "# ===== TEST OPTIMIZED PROMPTS =====\n",
    "print(\"‚úÖ OPTIMIZED PROMPTS (Concise & Efficient):\")\n",
    "optimized_total = 0\n",
    "\n",
    "for i, prompt in enumerate(lab_5_utils.optimized_prompts):\n",
    "    input_tokens, output_tokens, response = lab_5_utils.get_response(bedrock, US_ANTHROPIC_SONNET37_MODEL_ID, prompt)\n",
    "    cost = lab_5_utils.trim_prompt_calculate_cost(input_tokens, output_tokens)\n",
    "    optimized_total += cost\n",
    "\n",
    "    print(f\"  Query {i+1}:\")\n",
    "    print(f\"    Input: {input_tokens} tokens\")\n",
    "    print(f\"    Output: {output_tokens} tokens\")\n",
    "    print(f\"    Cost: ${cost:.6f}\")\n",
    "    print(f\"    Response: {response[:60]}...\")\n",
    "    print()\n",
    "\n",
    "print(f\"‚úÖ TOTAL OPTIMIZED: ${optimized_total:.6f}\")\n",
    "print()\n",
    "\n",
    "lab_5_utils.print_prompt_router_cost_savings(unoptimized_total, optimized_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Caching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When parts or whole of the prompt is reused in such scenarios we could benefit from using Bedrock optional feature of Prompt Caching. When using Prompt Caching the model skips recomputing parts of the prompt/context thus saving on both the response time as well as on the cost from the tokens. For example, if you have a chatbot where users can upload documents and ask questions about them, it can be time consuming for the model to process the document every time. With Prompt Caching is enabled future queries containing the document don't need to reprocess it.\n",
    "\n",
    "Below you will see examples how to apply Cache Message Checkpoints to optimze the latency and cost of invoking the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "importlib.reload(lab_5_utils)\n",
    "\n",
    "bedrock_runtime = boto3.client(\"bedrock-runtime\", region_name=REGION)\n",
    "\n",
    "# Pricing for Claude 3.5 Sonnet\n",
    "INPUT_COST = 3.00       \n",
    "OUTPUT_COST = 15.00     \n",
    "CACHE_READ_COST = 0.30  \n",
    "\n",
    "print(\"üöÄ PROMPT CACHING DEMO - Following Blog Pattern\\n\")\n",
    "\n",
    "# ===== WITHOUT CACHING =====\n",
    "print(\"üìä WITHOUT CACHING (Fresh conversation each time):\")\n",
    "total_cost_no_cache = 0\n",
    "lab_5_utils.clear_message_history()\n",
    "\n",
    "for i in range(3):\n",
    "    # Reset conversation for each call (no caching)\n",
    "    lab_5_utils.clear_message_history()\n",
    "\n",
    "    question = f\"Based on the policy, what's the return window for laptops? (Query {i+1})\"\n",
    "    response, usage = lab_5_utils.converse_with_context(question, bedrock_runtime, US_ANTHROPIC_SONNET37_MODEL_ID, add_context=True, cache=False)\n",
    "\n",
    "    cost = lab_5_utils.caching_calculate_cost(usage)\n",
    "    total_cost_no_cache += cost\n",
    "\n",
    "    print(f\"  Call {i+1}:\")\n",
    "    print(f\"    Input: {usage['inputTokens']:,}, Output: {usage['outputTokens']}\")\n",
    "    print(f\"    Cost: ${cost:.5f}\")\n",
    "    print(f\"    Response: {response[:100]}...\")\n",
    "    print()\n",
    "\n",
    "print(f\"üìä TOTAL WITHOUT CACHING: ${total_cost_no_cache:.5f}\\n\")\n",
    "\n",
    "# ===== WITH CACHING =====\n",
    "print(\"‚ö° WITH CACHING (Persistent conversation):\")\n",
    "lab_5_utils.clear_message_history()\n",
    "total_cost_with_cache = 0\n",
    "\n",
    "# First call - establish cache\n",
    "print(\"  Call 1 (Cache Miss - Setting up cache):\")\n",
    "question1 = \"Based on the policy, what's the return window for laptops? (Query 1)\"\n",
    "response, usage = lab_5_utils.converse_with_context(question1, bedrock_runtime, US_ANTHROPIC_SONNET37_MODEL_ID, add_context=True, cache=True)\n",
    "\n",
    "cost = lab_5_utils.caching_calculate_cost(usage)\n",
    "total_cost_with_cache += cost\n",
    "\n",
    "cache_read = usage.get(\"cacheReadInputTokens\", 0)\n",
    "print(f\"    Input: {usage['inputTokens']:,}, Output: {usage['outputTokens']}, CacheRead: {cache_read:,}\")\n",
    "print(f\"    Cost: ${cost:.5f}\")\n",
    "print(f\"    Response: {response[:100]}...\")\n",
    "print()\n",
    "\n",
    "# Subsequent calls - should hit cache\n",
    "for i in range(2):\n",
    "    print(f\"  Call {i+2} (Cache Hit):\")\n",
    "    question = f\"What about return fees for opened laptops? (Query {i+2})\"\n",
    "    response, usage = lab_5_utils.converse_with_context(question, bedrock_runtime, US_ANTHROPIC_SONNET37_MODEL_ID, add_context=False, cache=False)\n",
    "\n",
    "    cost = lab_5_utils.caching_calculate_cost(usage)\n",
    "    total_cost_with_cache += cost\n",
    "\n",
    "    cache_read = usage.get(\"cacheReadInputTokens\", 0)\n",
    "    print(f\"    Input: {usage['inputTokens']:,}, Output: {usage['outputTokens']}, CacheRead: {cache_read:,}\")\n",
    "    print(f\"    Cost: ${cost:.5f}\")\n",
    "    print(f\"    Response: {response[:100]}...\")\n",
    "    print()\n",
    "\n",
    "print(f\"‚ö° TOTAL WITH CACHING: ${total_cost_with_cache:.5f}\\n\")\n",
    "lab_5_utils.print_prompt_caching_results(total_cost_with_cache, total_cost_no_cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch events for inference provided use case allows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For use cases that require running through several line items and where response times are not critical considering Batch inferencing is optimal. Examples of such scenarios could be sentiment analysis or summarization of vast amounts of data. Amazon Bedrock supports Batch Jobs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import time\n",
    "importlib.reload(lab_5_utils)\n",
    "\n",
    "# Setup\n",
    "bedrock = boto3.client(\"bedrock-runtime\", region_name=REGION)\n",
    "\n",
    "# Amazon customer service prompts (realistic scenarios)\n",
    "customer_queries = [\n",
    "    \"How do I track my Amazon order?\",\n",
    "    \"What is the return policy for books?\",\n",
    "    \"How do I cancel my Amazon Prime membership?\",\n",
    "    \"I received a damaged item, what should I do?\",\n",
    "    \"Can I return an item without the original packaging?\",\n",
    "    \"How long does it take to get a refund on my credit card?\",\n",
    "    \"What items are not eligible for return on Amazon?\",\n",
    "    \"How do I contact Amazon customer service by phone?\",\n",
    "    \"Can I change my delivery address after placing an order?\",\n",
    "    \"What is Amazon's policy on late deliveries?\"\n",
    "]\n",
    "\n",
    "print(\"üõí AMAZON CUSTOMER SERVICE - ON-DEMAND vs BATCH\")\n",
    "print(f\"Processing {len(customer_queries)} customer inquiries...\")\n",
    "print()\n",
    "\n",
    "# ===== ON-DEMAND PROCESSING =====\n",
    "print(\"‚ö° ON-DEMAND PROCESSING (Immediate Response):\")\n",
    "on_demand_total = 0\n",
    "\n",
    "for i, query in enumerate(customer_queries):\n",
    "    input_tokens, output_tokens = lab_5_utils.process_on_demand(bedrock, US_ANTHROPIC_SONNET37_MODEL_ID, query)\n",
    "    cost = lab_5_utils.batch_calculate_cost(input_tokens, output_tokens, is_batch=False)\n",
    "    on_demand_total += cost\n",
    "\n",
    "    print(f\"  Query {i+1}: {input_tokens}‚Üí{output_tokens} tokens = ${cost:.6f}\")\n",
    "\n",
    "print(f\"  TOTAL ON-DEMAND: ${on_demand_total:.6f}\")\n",
    "print()\n",
    "\n",
    "# ===== BATCH PROCESSING =====\n",
    "print(\"üì¶ BATCH PROCESSING (Delayed, but Cheaper):\")\n",
    "batch_results = lab_5_utils.process_batch(bedrock, US_ANTHROPIC_SONNET37_MODEL_ID, customer_queries)\n",
    "batch_total = 0\n",
    "\n",
    "for i, (input_tokens, output_tokens) in enumerate(batch_results):\n",
    "    cost = lab_5_utils.batch_calculate_cost(input_tokens, output_tokens, is_batch=True)\n",
    "    batch_total += cost\n",
    "\n",
    "    print(f\"  Query {i+1}: {input_tokens}‚Üí{output_tokens} tokens = ${cost:.6f}\")\n",
    "\n",
    "print(f\"  TOTAL BATCH: ${batch_total:.6f}\")\n",
    "print()\n",
    "\n",
    "lab_5_utils.print_batch_results(on_demand_total, batch_total, customer_queries)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
